### CrawlSpider

- 实现网站的全站数据爬取

  - 就是将网站中所有页码对应的页面数据进行爬取。

- crawlspider其实就是scrapy封装好的一个爬虫类，通过该类提供的相关的方法和属性就可以实现全新高效形式的全站数据爬取。

- 使用流程：

  - 新建一个scrapy项目

  - cd 项目

  - 创建爬虫文件（*）：

    - scrapy genspider -t crawl spiderName www.xxx.com

    - 爬虫文件中发生的变化有哪些？
      - 当前爬虫类的父类为CrawlSpider
      - 爬虫类中多了一个类变量叫做rules
        - LinkExtractor：链接提取器
          - 可以根据allow参数表示的正则在当前页面中提取符合正则要求的链接
        - Rule：规则解析器
          - 可以接收链接提取器提取到的链接，并且对每一个链接进行请求发送
          - 可以根据callback指定的回调函数对每一次请求到的数据进行数据解析
      - 思考:如何将一个网站中所有的链接都提取到呢？
        - 只需要在链接提取器的allow后面赋值一个空正则表达式即可
      - 目前在scrapy中有几种发送请求的方式？
        - start_urls列表可以发送请求
        - scrapy.Request()
        - scrapy.FormRequest()
        - Rule规则解析器

- 注意：

  - 链接提取器和规则解析器是一一对应的（一对一的关系）
  - 建议在使用crawlSpider实现深度爬取的时候，需要配合手动请求发送的方式进行搭配！

  

- ```
  USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'
  ```

- 爬取不同页码下的标题数据

  - ```python
    import scrapy
    from time import sleep
    from scrapy.linkextractors import LinkExtractor
    from scrapy.spiders import CrawlSpider, Rule
    #LinkExtractor链接提取器:可以根据指定的规则进行链接的提取
    #Rule规则解析器:可以根据指定规则对请求到的数据进行数据解析
    class SunSpider(CrawlSpider):
        name = 'sun'
        # allowed_domains = ['www.xxx.com']
        start_urls = ['https://wz.sun0769.com/political/index/politicsNewest?id=1&page=1']
    
        #link就表示创建出来的一个链接提取器
        #allow参数后面要跟一个正则表达式，就可以作为链接提取的规则
        #link首先会去start_urls表示的页面中进行链接的提取
        link = LinkExtractor(allow=r'id=1&page=(\d+)')
        rules = (
            #创建了一个规则解析器
            #链接提取器提取到的链接会发送给Rule这个规则解析器
            #规则解析器接收到链接后，会对链接进行请求发送，并且根据callback指定的函数进行数据解析
            Rule(link, callback='parse_item', follow=True),
        )
    
        #解析方法的调用次数取决于link提取到的链接的个数
        def parse_item(self, response):
            li_list = response.xpath('/html/body/div[2]/div[3]/ul[2]/li')
            for li in li_list:
                title = li.xpath('./span[3]/a/text()').extract_first()
                print(title)
    ```

- 爬取页码对应页面的标题数据和详情页中的详细内容数据（深度爬取）

  - ```python
    import scrapy
    from time import sleep
    from scrapy.linkextractors import LinkExtractor
    from scrapy.spiders import CrawlSpider, Rule
    from crawlPro.items import CrawlproItem
    #LinkExtractor链接提取器:可以根据指定的规则进行链接的提取
    #Rule规则解析器:可以根据指定规则对请求到的数据进行数据解析
    class SunSpider(CrawlSpider):
        name = 'sun'
        # allowed_domains = ['https://wz.sun0769.com']
        start_urls = ['https://wz.sun0769.com/political/index/politicsNewest?id=1&page=1']
    
        #链接提取器和规则解析器一定是一对一关系
            #提取页码链接
        link = LinkExtractor(allow=r'id=1&page=\d+')
    
        rules = (
            #解析页面对应页面中的标题数据
            Rule(link, callback='parse_item', follow=False),
        )
    
        #解析方法的调用次数取决于link提取到的链接的个数
        def parse_item(self, response):
            li_list = response.xpath('/html/body/div[2]/div[3]/ul[2]/li')
            for li in li_list:
                title = li.xpath('./span[3]/a/text()').extract_first()
                detail_url = 'https://wz.sun0769.com' + li.xpath('./span[3]/a/@href').extract_first()
                item = CrawlproItem()
                item['title'] = title
                yield scrapy.Request(url=detail_url, callback=self.parse_detail,meta={'item':item})
    
        def parse_detail(self,response):
            item = response.meta['item']
            detail_content = response.xpath('/html/body/div[3]/div[2]/div[2]/div[2]//text()').extract()
            item['content'] = detail_content
            yield item
    ```

    

### 分布式

- 分布式在日常开发中并不常用，只是一个噱头！

- 概念：

  - 可以使用多台电脑搭建一个分布式机群，使得多台对电脑可以对同一个网站的数据进行联合且分布的数据爬取。

- 声明：

  - 原生的scrapy框架并无法实现分布式操作！why？
    - 多台电脑之间无法共享同一个调度器
    - 多台电脑之间无法共享同一个管道

- 如何是的scrapy可以实现分布式呢？

  - 借助于一个组件：scrapy-redis
  - scrapy-redis的作用是什么？
    - 可以给原生的scrapy框架提供可被共享的调度器和管道！
    - 环境安装：pip install scrapy-redis
      - 注意：scrapy-redis该组件只可以将爬取到的数据存储到redis数据库

- 编码流程（重点）：

  - 1.创建项目

  - 2.cd 项目

  - 3.创建基于crawlSpider的爬虫文件

    - 3.1 修改爬虫文件
      - 导包：from scrapy_redis.spiders import RedisCrawlSpider
      - 修改当前爬虫类的父类为 RedisCrawlSpider
      - 将start_urls替换成redis_key的操作
        - redis_key变量的赋值为字符串，该字符串表示调度器队列的名称
      - 进行常规的请求操作和数据解析

  - 4.settings配置文件的修改

    - 常规内容修改（robots和ua等），先不指定日志等级

    - 指定可以被共享的管道类

      - ```
        ITEM_PIPELINES = {
            'scrapy_redis.pipelines.RedisPipeline': 400
        }
        ```

    - 指定可以被共享的调度器

      - ```
        # 使用scrapy-redis组件的去重队列
        DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
        # 使用scrapy-redis组件自己的调度器
        SCHEDULER = "scrapy_redis.scheduler.Scheduler"
        # 是否允许暂停
        SCHEDULER_PERSIST = True
        ```

    - 指定数据库

      - ```
        REDIS_HOST = '127.0.0.1'
        REDIS_PORT = 6379
        ```

  - 5.修改redis数据库的配置文件（redis.windows.conf）

    - 在配置文件中改行代码是没有没注释的：

      - ```
        bind 127.0.0.1
        #将上述代码注释即可（解除本机绑定，实现外部设备访问本机数据库
        
        如果配置文件中还存在：protected-mode = true，将true修改为false，
        修改为false后表示redis数据库关闭了保护模式，表示其他设备可以远程访问且修改你数据库中的数据
        ```

  - 6.启动redis数据库的服务端和客户端

  - 7.运行项目,发现程序暂定一直在等待，等待爬取任务

  - 8.需要向可以被共享的调度器的队列（redis_key的值）中放入一个起始的url

    - 在redis数据库的客户端执行如下操作：

      - lpush 队列名称 起始的url

      - 起始url：https://wz.sun0769.com/political/index/politicsNewest?id=1&page=1

        



